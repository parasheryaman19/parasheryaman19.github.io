<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Tensorlog &middot; Martin Jedwabny
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/general.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700&display=swap">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700,700i&display=swap">
  <link href="https://fonts.googleapis.com/css?family=Zilla+Slab+Highlight:400,700&display=swap" rel="stylesheet">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/logo.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-mj">

    <div class="sidebar">
  <div class="container sidebar-sticky">

    <div class="sidebar-about">
        <a href="/">
          <h2>
            Martin Jedwabny
          </h2>
        </a>
      <p class="lead">
        [PhD. student] 
        @University of Montpellier. 
        [Team member] 
        @INRIA GraphIK.
      </p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      <a class="sidebar-nav-item active" href="/blog">Blog</a>
      
      <a class="sidebar-nav-item" href="https://github.com/martinjedwabny">GitHub</a>
      <!-- <span class="sidebar-nav-item">Currently v2.1.0</span> -->
    </nav>

    <p>&copy; 2020. All rights reserved.</p>
  </div>
</div>


    <div class="logo-tag">MJ</div>

    <div class="content container">
      <div class="post">
  <h1 class="post-title">Tensorlog</h1>
  <span class="post-date">15 Oct 2019</span>
  
    <span class="post-tag">Logic</span>
  
    <span class="post-tag">Learning</span>
  
    <span class="post-tag">Query answering</span>
  
    <span class="post-tag">Neural-Symbolic</span>
  
  <p>TensorLog <a href="#cohenYM17">(Cohen et al., 2017)</a> is an implementation of probabilistic knowledge bases that (i) leverages deep learning to answer queries efficiently, by (ii) restricting the first order logic language to a class called p-tree-DKG, and (iii) learns weights on knowledge base facts.</p>

<!--more-->

<h2 id="how">How</h2>

<p>The paper explains that the problem with past approaches is that they rely on grounding first order clauses (either for training or answering queries). For example, given a (Prolog-like) rule:</p>

<script type="math/tex; mode=display">p(X,Y) \text{ :- } q(Y,Z),r(Z,Y)</script>

<p>Representing the formula:</p>

<script type="math/tex; mode=display">\forall X \forall Y \ (p(X,Y) \lor \neg (q(Y,Z) \land r(Z,Y)))</script>

<p>And a logic vocabulary with a set of constants <script type="math/tex">\mathcal{C}</script> of size <script type="math/tex">n</script>, the amount of clauses generated by the grounding of this clause would be <script type="math/tex">n^3</script>, as each variable can be replaced by any constant. It becomes apparent that with larger amounts of variables in the clause, grounding them can quickly become untractable.</p>

<p>Let us quickly define the following: a database is a set of ground facts <script type="math/tex">\mathcal{DB} = \{f_1, ..., f_N\}</script>, and a theory <script type="math/tex">\mathcal{T}</script> is as a set of Horn clauses (Prolog like), which can also be seen as knowledge base rules (as in the example above).</p>

<!-- An augmented theory $$\mathcal{T}^{+ \mathcal{DB}}$$ is obtained by adding to the theory the unit clauses of the database as rules with an empty body. $$Model(\mathcal{DB},\mathcal{T})$$ is used to denote the universal model i.e. the facts that are present in the database or are deductible through the rules in the theory.  -->

<p>The proof methodology here is backwards chaining i.e. top-down unification of literals through rules. Then, a proof tree for a query <script type="math/tex">Q</script> is a graph where nodes contain tuples (S,L) consisting of a solution S (to a query or a successive unification state) and a list L of (non-grounded) literals to unify, where <script type="math/tex">(Q, [ Q ])</script> is the root.</p>

<p><img src="https://martinjedwabny.github.io/public/img/tensorlog/1.png" alt="useful image" /></p>

<p>Now, in order to incorporate probabilistic reasoning to our scheme, we have to assign weights to our facts and rules and provide an extension for query answering through proof trees.</p>

<p>A stochastic logic program (<strong>SLP</strong>) is a tuple <script type="math/tex">(\mathcal{DB}, \mathcal{T}, \Theta)</script> where <script type="math/tex">\Theta</script> maps literals and rules to non-negative real numbers.
Given a query <script type="math/tex">Q</script> with proof graph <script type="math/tex">G</script> and an answer <script type="math/tex">f</script>, the weight of <script type="math/tex">f</script> with respect to <script type="math/tex">Q</script> is:</p>

<script type="math/tex; mode=display">w_Q(f) = \sum\limits_{v_0=(Q,[Q])\rightarrow ... \rightarrow v_n=(f,[])} \prod\limits_{i=0}^{n-1} \Theta_{r_{v_i,v_{i+1}}}</script>

<p>Where <script type="math/tex">r_{v_i,v_{i+1}}</script>  is the rule used between nodes <script type="math/tex">v_i</script> and <script type="math/tex">v_{i+1}</script> in <script type="math/tex">G</script>.</p>

<p>Then, we can normalize all the answers <script type="math/tex">\{ f_1,...,f_k \}</script> to the query as:</p>

<script type="math/tex; mode=display">P(f_i | Q) = \frac{w_Q(f_i)}{\sum\limits_{j=1}^{k} w_Q(f_j)}</script>

<p><strong>(Theorem 1)</strong> Computing <script type="math/tex">P(f|Q)</script> for all possible answers <script type="math/tex">f</script> of
the query <script type="math/tex">Q</script> is #P-hard, even if there are only two such answers, the theory contains only
two non-recursive clauses, and the knowledge graph contains only 13 facts.</p>

<p>To bypass this, the authors propose a series of restrictions that lead to more efficient query answerings.</p>

<p>A polytree-limited stochastic deductive knowledge graph (<strong>ptree-SDKG</strong>) is a SLP where:</p>
<ul>
  <li><script type="math/tex">\mathcal{DB}</script> and <script type="math/tex">\mathcal{T}</script> have no function symbols.</li>
  <li><script type="math/tex">\mathcal{DB}</script> and <script type="math/tex">\mathcal{T}</script> have only unary and binary predicates.</li>
  <li>Theory <script type="math/tex">\mathcal{T}</script> clauses have weight 1, i.e. <script type="math/tex">\forall r \in \mathcal{T}, \ \Theta(r)=1</script>.</li>
  <li>Theory <script type="math/tex">\mathcal{T}</script> clauses are not recursive.</li>
  <li>Theory <script type="math/tex">\mathcal{T}</script> clauses <script type="math/tex">r = A \text{ :- } B_1, ... , B_k</script> satisfy that, given a graph with nodes <script type="math/tex">B_i</script> and edges <script type="math/tex">(B_i,B_j)</script> where <script type="math/tex">i \neq j</script> for each pair of predicates that share a variable, then the graph has no cycles (polytree restriction).</li>
</ul>

<p>An example:</p>

<p><img src="https://martinjedwabny.github.io/public/img/tensorlog/2.png" alt="useful image" /></p>

<p><strong>(Theorem 2)</strong> For any ptree-SDKG, <script type="math/tex">P(f\mid Q)</script> can be computed in linear time in the size of <script type="math/tex">\mathcal{T}</script> and <script type="math/tex">\mathcal{DB}</script>.</p>

<p>With these restrictions, it holds that:</p>

<script type="math/tex; mode=display">w'_Q(f) = \sum\limits_{v_0=(Q,[Q])\rightarrow ... \rightarrow v_n=(f,[])} \prod\limits_{ \{ f \in \mathcal{DB} \mid f \text{ is used in the path} \} } \Theta_{f}</script>

<script type="math/tex; mode=display">P(f_i | Q) = \frac{w'_Q(f_i)}{\sum\limits_{j=1}^{k} w'_Q(f_j)}</script>

<p>Meaning that we can compute the normalized answer to a query <script type="math/tex">Q</script> only by looking at the facts that were used in the proof graph to unify, as they are the only ones with weight different than 1.</p>

<h2 id="conclusion">Conclusion</h2>
<ul>
  <li>Identified a family of probabilistic deductive databases called polytree-limited stochastic deductive knowledge graphs (ptree-SDKGs) which are tractable for query answering and learning weights of facts.</li>
  <li>Presented an algorithm for performing inference for ptree-SDKGs.</li>
  <li>Even if the logic language is maximally expressive, it seems highly restrictive, and all it can learn is weights for facts, not rules, then, query answering is efficient because it disallows recursion and imposes further restrictions.</li>
  <li>Implemented the framework, called Tensorlog, which is available in <a href="https://github.com/TeamCohen/TensorLogs">Github</a>.</li>
</ul>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="cohenYM17">Cohen, W. W., Yang, F., &amp; Mazaitis, K. (2017). TensorLog: Deep Learning Meets Probabilistic DBs. <i>CoRR</i>, <i>abs/1707.05390</i>. http://arxiv.org/abs/1707.05390</span></li></ol>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2019/11/25/srl/">
            Statistical relational learning
            <small>25 Nov 2019</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2019/10/23/markov/">
            Markov Logic Networks
            <small>23 Oct 2019</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2019/10/23/bay-markov/">
            Bayesian and Markov networks
            <small>23 Oct 2019</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
